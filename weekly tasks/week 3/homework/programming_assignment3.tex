\documentclass[11pt, a4paper]{article}
\usepackage{hyperref}
\hypersetup{colorlinks=true, urlcolor=blue, breaklinks=true}
\newcommand{\angles}[1]{$\langle$#1$\rangle$}

%%%%% ADD THIS TO YOUR PREAMBLE BEFORE LOADING Alegreya
\DeclareFontFamily{T1}{Alegreya-LF}{}
\newcommand{\adjustalegreya}{\fontdimen5\font=\fontcharht\font`x }
\makeatletter
\let\Alegreya@@scale\@empty
%%% uncomment the next line if you want to scale the font,
%%% changing the value to what suits you
% \def\Alegreya@@scale{s*[0.9]}%
\makeatother

\DeclareFontShape{T1}{Alegreya-LF}{k}{n}{
      <-> \Alegreya@@scale Alegreya-Black-lf-t1
}{\adjustalegreya}

\DeclareFontShape{T1}{Alegreya-LF}{k}{it}{
      <-> \Alegreya@@scale Alegreya-BlackItalic-lf-t1
}{\adjustalegreya}

\DeclareFontShape{T1}{Alegreya-LF}{k}{sl}{
      <-> ssub * Alegreya-LF/k/it
}{\adjustalegreya}

\DeclareFontShape{T1}{Alegreya-LF}{b}{n}{
      <-> \Alegreya@@scale Alegreya-Bold-lf-t1
}{\adjustalegreya}

\DeclareFontShape{T1}{Alegreya-LF}{b}{it}{
      <-> \Alegreya@@scale Alegreya-BoldItalic-lf-t1
}{\adjustalegreya}

\DeclareFontShape{T1}{Alegreya-LF}{b}{sl}{
      <-> ssub * Alegreya-LF/b/it
}{\adjustalegreya}

\DeclareFontShape{T1}{Alegreya-LF}{m}{n}{on the spot
      <-> \Alegreya@@scale Alegreya-Regular-lf-t1
}{\adjustalegreya}

\DeclareFontShape{T1}{Alegreya-LF}{m}{it}{
      <-> \Alegreya@@scale Alegreya-Italic-lf-t1
}{\adjustalegreya}

\DeclareFontShape{T1}{Alegreya-LF}{m}{sl}{
      <-> ssub * Alegreya-LF/m/it
}{\adjustalegreya}

\DeclareFontShape{T1}{Alegreya-LF}{bx}{sl}{
      <-> ssub * Alegreya-LF/b/sl
}{\adjustalegreya}

\DeclareFontShape{T1}{Alegreya-LF}{bx}{n}{
      <-> ssub * Alegreya-LF/b/n
}{\adjustalegreya}

\DeclareFontShape{T1}{Alegreya-LF}{bx}{it}{
      <-> ssub * Alegreya-LF/b/it
}{\adjustalegreya}

%%%% NOW YOU CAN SAFELY LOAD Alegreya (don't pass a scale option)
\usepackage{Alegreya}
\usepackage{amsmath}

\title{Programming Assignment 3 -- Basic Probability \\[2mm]
\large{2017-2018, Master of Logic, University of Amsterdam}}
\author{Instructors: Jakub Dotla\v{c}il, Alexandre Cremers and Bas Cornellisen}
\date{Submission deadline: Wednesday, April 25, 8 pm}

\begin{document}
\maketitle

\paragraph{Note:} if the assignment is unclear to you or if you get stuck, do not hesitate to contact \href{mailto:j.dotlacil@uva.nl}{Jakub} or \href{mailto:bjmcornelissen@gmail.com}{Bas}. You can also post a question on Canvas.

\section{Assignment}
This week we will look at one of the most useful applications of natural language processing techniques: information retrieval. 
Suppose you have a vast \emph{corpus} of documents such as books, articles or Wikipedia lemma's.
You are looking for the document that most resembles your \emph{query} ``to be or not to be''. 
How can we automatically retrieve the document that is `closest to', or `most similar to' or `most relevant for' this query?
That's what we look at today.



We will try to quantify the `similarity' between each document and our query. 
The idea is to represent fragments of text (documents or queries) as \emph{vectors}.\footnote{You can read more about this so called vector-space model on \href{https://en.wikipedia.org/wiki/Vector_space_model}{Wikipedia}} 
Vectors are simply lists of numbers, such as $(4, 4.2, 1.2, 6, 7)$. 
This particular vector was five-dimensional, since it has five entries. 
Vectors with two dimensions such as $(7, 2)$ can be thought of as points in a plane, or as an arrow pointing from the origin to that point in the plane.
The `similarity' between two vectors $a$ and $b$ can be measured by computing the \emph{angle} between such vectors. 
Well, the cosine of that angle, to be precise, hence the name \emph{\href{https://en.wikipedia.org/wiki/Cosine_similarity}{cosine-similarity}}. 
For now you only have to know that we can measure the similarity between two vectors (lists of numbers) $a=(a_1, a_2, \dots, a_n)$ and $b = (b_1, b_2, \dots, b_n)$ as follows:
\begin{align}
  \text{similarity}(a, b) 
    = \frac{ a_1 \cdot b_1 \;+\; a_2 \cdot b_2 \;+\; \dots \;+\; a_n \cdot b_n}%
      {\text{norm}(a) \cdot \text{norm}(b)}
\end{align}
where the norm (or length if you like) is
\begin{align}
  \text{norm}(a) = \sqrt{a_1^2 + a_2^2 + \dots + a_n^2}
\end{align}



Good. 
So how do we turn text into vectors?
There are many possible ways to do so, and we will only look at the simplest possibility (the most naive approach): use the vector of word-frequencies.
This is not completely trivial.
It is quite likely that a document contains a word that is not in any of the other documents, but to compare vectors they have to be of the same length.
Differently put, they have to live in the same space.
We therefore first construct a \emph{vocabulary}: the list of the, say, 5000 most common words throughout the entire corpus.
We can then count the number of times every word \emph{in the vocabulary} occurs in a given document. 
If you put those $5000$ frequencies in a list, you get a 5000-dimensional vector representing your document (or query).
It's quite likely that it contains many zero's, but that's fine.\footnote{But note that if your vector contains \emph{only} zeros, you might run into problems...} 
And once we have vectors, we can compute cosine similarities, and identify the document with the highest similarity to our query.


Word frequency vectors are probably the simplest, or most naive, vector representations. 
You can easily improve on this in many ways, and we encourage you to try so (but perhaps after finishing the homework). 
The Python file walks you through the assignment in nine steps, just like the in class exercises. 
The file contains lots of additional explanation, but if something is unclear, do not hesitate to post your questions on the forum. Here are some practical remarks:

\begin{itemize}
  \item Note that you need to download and unzip the \texttt{corpus.zip} file and move the \texttt{corpus} folder to the folder you are working in. 
  \item You will have to use \texttt{Counter} objects a lot; see assignment 2 for more info.
  \item You are not allowed to use \texttt{numpy}, a library for working with vectors we will discuss later in the course. Part of this assignment is to learn how to work with Python's data structures. Also, it will illustrate why you normally \emph{do} want to use \texttt{numpy} in these situations.
  \item Our implementation will not be very efficient (we read out all the files twice, for example), but we hope this structure makes it easier to follow \emph{what} the code should be doing. 
\end{itemize}


\section{Grading}
TO DO 

%\begin{itemize}
%\item[1 point] When the program starts it asks the user for a path to a file (including the file name). It is ok for the program to crash if this path does not exist.
%\item[2 points] The program reads the text file only once and stores all relevant information before proceeding to three functions.Depending on your computer reading a file can take several seconds.
%\item[1 point] The program generates a dictionary using the function collect\_frequencies, with words as keys and integers as values.
%\item[2 points] The program prints the number $n$ of most frequent words to the screen as a list in which each line looks as follows:
%$$ word~~~wordCount $$
%If there are several words of the same count, they are ordered alphabetically (1 point).
%\item[2 points] The count of specific words is printed in the same format as the above two items (1 point). If a word does not occur in the text, the program prints \textit{Sorry, this
%word does not occur in the text} (1 point).
%\item[1 point] The function find\_frequent\_words has an optional argument, which has the default value of 50 (that is, by default, 50 most frequent words are returned. The default value can be changed and the function still works correctly.
%\item[1 point] The function find\_word has an optional argument, word. If no value is specified for this argument, the function selects a word from the file at random. If the value is specified, the function correctly returns only the information about that word.
%\end{itemize}


\end{document}
